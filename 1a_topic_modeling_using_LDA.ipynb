{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling using Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "| Key              | Value                                                                                                                                                                                               |\n",
    "|:-----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Course Codes** | BBT 4106, BCM 3104, and BFS 4102                                                                                                                                                                    |\n",
    "| **Course Names** | BBT 4206: Business Intelligence II (Week 7-9 of 13),<br/>BFS 4102: Advanced Business Data Analytics (Week 7-9 of 13) and <br/>BCM 3103: Business Intelligence and Data Analytics (Week 10-12 of 13) |\n",
    "| **Semester**     | April to July 2025                                                                                                                                                                                  |\n",
    "| **Lecturer**     | Allan Omondi                                                                                                                                                                                        |\n",
    "| **Contact**      | aomondi@strathmore.edu                                                                                                                                                                              |\n",
    "| **Note**         | The lecture contains both theory and practice. This notebook forms part of the practice. This is intended for educational purpose only. Recommended citation: [BibTex](https://github.com/course-files/SentimentAnalysis/raw/refs/heads/main/RecommendedCitation.bib)                            |\n",
    "\n",
    "**Business context**: A business has set a strategic objective *to increase the monthly average customer rating to 3.8/5 by the end of the current financial year*. The business tracks two Key Performance Indicators (KPIs) from the customer perspective:\n",
    "\n",
    "1. **Lagging KPI**: Monthly average customer rating\n",
    "2. **Leading KPI**: List of key themes in customer reviews. This will then be followed up with sentiment analysis to identify how customers feel about these themes.\n",
    "\n",
    "The business wants to leverage Natural Language Processing (NLP) to identify the key themes present in customer reviews. This will enable it to better understand the qualitative aspects of customer feedback. By applying topic modeling techniques to historical review data, the business can identify what customers are talking about (the topics), which can then be followed up with sentiment analysis to identify how customers feel about these topics. That is, whether their comments are positive, negative, or neutral.\n",
    "\n",
    "**Dataset:** The original dataset by **Ott and Arvidsson (2023)** consists of 878,561 reviews (1.3GB) from 4,333 hotels crawled from **TripAdvisor ([https://www.tripadvisor.com/](https://www.tripadvisor.com/))**.\n",
    "- Some reviews are written in French. Source: [https://www.cs.cmu.edu/~jiweil/html/hotel-review.html](https://www.cs.cmu.edu/~jiweil/html/hotel-review.html) or [https://www.kaggle.com/datasets/joebeachcapital/hotel-reviews](https://www.kaggle.com/datasets/joebeachcapital/hotel-reviews).\n",
    "- We use a scaled-down version of the dataset that contains 50,000 reviews for the sake of performance and efficiency in a lab setting for educational purposes.\n",
    "\n",
    "| Feature            | Description                                                                           |\n",
    "|--------------------|---------------------------------------------------------------------------------------|\n",
    "| `date`             | Indicates the date when the review was written                                        |\n",
    "| `offering_id`      | Indicates the ID of the hotel that the customer stayed in                             |\n",
    "| `date_stayed`      | Indicates the date when the customer stayed at the hotel                              |\n",
    "| `text`             | Contains the review text                                                              |\n",
    "| `rating_overall`   | Overall rating given by the customer (1 to 5 stars; 1 is the worst and 5 is the best) |\n",
    "| `is_english`       | Indicates whether the review is written in English (`True`) or not (`False`)          |\n",
    "| `author_username`  | Indicates the username of the customer who wrote the review                           |\n",
    "| `author_location`  | Indicates the location of the customer who wrote the review                           |"
   ],
   "id": "e1a2dd42fa4236d3",
   "attachments": {
    "b33c8e0b-6ccc-498b-bb0a-5c3fc54412ca.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAAeCAIAAACzEOfgAAACG0lEQVR4Xu2XzU7CQBSF+1x9Do0LjUYlNYgYCUEIQUFQ4pKncOXOpC/TjW+g8tcW2uJM/2jnzrTgdEiccHJY9Pb2kvNlOpkqn3vlSUG/1V5s7RnlS1pGb+8fRVlmRmTpT5Kckeet+C09owIkOSPX9fgtOSPH9fjNYmQYOpKRLvKo8IH5wowcl98sRrqmKIo6Ki5T4QPzhbItHZffkjNaLB1+UxgZI1VJSdPX/2voWnxX1ZKRAYRgDno4c6BAoWz2wuE3hdEK7xx+LATB30Si3EHYsDoKYCXy+pTi68QVc6BYsRgNXl5j9wbDh26/3Xm8b7Vr9QZsZjDCAqsiqqUWFQ6eaEpck7eoAwULZbPsJdX9wbD79Nx56DXbnXqjdVurX1eqsC3wFowAIgqJsOCvGsrTO2dkWkuWG028cG6qd1flykVJgw2xN2cUxTeSiractfwHiZcwru+c0dxaZLiklc/OL49PTuGtpDdnFIcHStMI+8h9GQ4ULpRtZi6yfXB4BIuEN2cE3iuqgi4VtsKBwoWyTec2v7dkBJZHWjFHCBQOFC7RjGBIGiRUUtenpFQDyYQyULRQtsnM5jeLUbit+OcZ+vkIHJAIKiRS2kCxQtnGU4vfTEY4lM+AOBKhaljG+44Wf6WS6wb3AkqUgQKFsv1MLX5nMPr3wowmJr8lZ/Q9MfktOaOvsclv6RnN+S05o6IsLaMCFTLaK1u/j0RbORCO4TYAAAAASUVORK5CYII="
    },
    "72fcb856-7aab-4ef7-8a78-8d6a7a18721e.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAAeCAIAAACzEOfgAAACwUlEQVR4Xu2Z3W7TMBiGc12+DhAHIBAwGY0xxFSVTVX3w7pNHOYqdsQZUm4mJ9wBsHZN2iYN/kvi38SJUy1EffXuoLbz2d8T+6u3eb/2qpOHfrK9zNozqtdgGd3/+NmVh8xIbmqlgTPabjN3D55RBxo4ozTdunvnjMIAep4Hg1Du2L1Qbkm6dXe3jMIwQBJwPDGjJHV3t4wID+DvgkeLmCi3TZK6+z9gFPrAI1tR7qgTym29SdxtYMQOCD4peIHl+kIfArJkLADKM8QyKZU/k4fih5WRsbgodEzgF31oDtTvB6Eddxy1CItyW60Td1cxAmQ6ACFkO4MmSJeMSo/4fnEpKvpxVWJZ6RjhyICMYzS4vUenhmQKNoflzmSvic6PApsYzW6/F76a3Z1fXk+mF1/PJiejsTq4hpFcatEakPj1ivnrz5qOkTBGeIj0871Kg0mMPjcS5RavNlpfz+4uv91Mz69OJ9PR+Ozzyejj0bE6jLqKUX0FkBOwZSREFoLIEeXnzVLHodyieGPy+BRvnE/HXz4cHr07gOqAwm0Y4TNARU7KbhkpDQZpxqHclvG6wgfw8M3b9y9fvVa7eDdiVNZZgKsUpPWxU0Z5jMb1SDM1yu0xWlf72fMXaqPkBoxIm+YcdMwoY98FTPKXnklylIwwWixX7rZnpFmEwkT+XLRZMyIflA1sI3XJvWBEE+baNGPaMbLaOqKUxWBG88eVu+0Z5WeN3I5oucZXAXFZdAy5oFTdj8yMMv5ySUUuUjbQGCX+fvSwiN3dgJGwfHrF1uybYkj+dENG5CLPKjaVT7nLi9FKvmf/XcTuNjB6KunLkf6N1Qozmkfu7hkjLQ3NbrUSyu3PPHJ3zxgJRY9ckNhvdIgb7dJKDxDl9vshcnffGOGSwv9poUHJVkQYLd3dP0bd6V75N1lrD5ZRh2KM9qrWPwAsBdVaqqb2AAAAAElFTkSuQmCC"
    },
    "63e2fc75-dc89-43f5-a080-e7c351d549e8.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAAeCAIAAACzEOfgAAAC5ElEQVR4Xu2Zz27TQBDG/Vx+DhAHEAioFpVSRBWFVlFKSwMVRz9FT9yQ/DK+8AaFJI2dxE7YP7POemfsyl5XxVY+TQ/5vJmd+Xl37aje773uk8f/tnuVa8/ofvWW0c3PX21FnxnZViP1nNFms3WP3jNqQT1nlGUb9/jvGEUh8zyPhZF9ob54b2m2cQ8HRiHzPSk/gIawU18tM0oz92jMSLbCYYRcqh/sPKYiyWidZu7RlFEUiCXDwirnkaQK4ZXw3lbr1D0qGPFV78Pe8XzfWP1qwezkBwFy9GCRJHeZtQVhW+lBiq82lXLyRiKzFjkmDIxJfDFPGKl1tFyl7lHGCG6GmC4UJYhPuvMokpZ0xMbiBSHHSKJy6D7MhSZx+Co3Y0CQYiTG+DIPZDEOPJWEiSJ4GWq4ulrGaPL9Rx6Xk+vzi6vR+Mvns9HJYIgHlzKCyszbrkrZrQ+0s7BT7FYPsduzD2iKUaGUQiV2StPgvSXLNRlXk+uLr9/G55eno/FgePbxZPD+6BgPU0EzQt1pL68GE0EOkcTqiBhhmyirlQQxMr7Pe4uTdVkMT8XC+XD86d3h0ZsDhgfkQTLCE2szrxbXbjuQRO0BreKYh2BUXEeLZFURB+zw1eu3z1+8xJfMIBlZ+0qpLiPYSITaZKSLLTmP7uJVdTx5+gybVpCM0M3JzRqMyCRFtcJoq7hoGQ893tt8sXQPkhFZfN3zyP5MiJqmJiP5wU4BelhGqjJ8FO4cXDtykCEt8QgvbBO7wUaMyNXKe5vdLd2DZpRTIt+PdtcrGeVJ1PsR8YLUAqNt4S1VShYtrvLepvPEPcoYweR69sJ7tryIaseOdI0kKEsbjCJxA+HEhmenSCC+wnv7O0/co4JRFwRbzXIBs2A0i92j44zIlagXGu/tzyx2j44zUpDgV6x8QYJjj3MT6+h2GrtH1xmJEw8eKaD8yBbn0e104R7dZ1SuG/RvssbRW0YtChjtVa1/8kSOkFcSX3EAAAAASUVORK5CYII="
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Import the necessary libraries",
   "id": "c36da7e954955463"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose**: This chunk imports all the necessary libraries for data analysis, machine learning, and visualization.\n",
    "\n",
    "1. **For File and system operations [urllib3](https://urllib3.readthedocs.io/en/stable/)**\n",
    "    - `urllib.request` is used for opening and downloading data from URLs.\n",
    "    - `os` provides functions for interacting with the operating system, such as file and directory management.\n",
    "    - The `import sys` statement allows access to Python's system-specific parameters and functions, such as command-line arguments and the interpreter environment.\n",
    "    - `sys` is imported to check if the code is running in Google Colab or not, which can affect how files are downloaded or saved.\n",
    "\n",
    "2. **For data manipulation - [pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html) and [numpy](https://numpy.org/doc/stable/index.html):**\n",
    "    - `pandas as pd`: For loading the dataset, creating and managing DataFrames, data manipulation and analysis using DataFrames\n",
    "    - `numpy as np`: For numerical operations and array manipulations\n",
    "\n",
    "3. **For text preprocessing - [re](https://docs.python.org/3/library/re.html)**\n",
    "    - `re`: For regular expression operations to clean and preprocess text data\n",
    "\n",
    "4. **For topic modeling - [scikit-learn](https://scikit-learn.org/stable/supervised_learning.html)**\n",
    "    - `CountVectorizer` converts a collection of text documents into a matrix of token counts for machine learning.\n",
    "    - `LatentDirichletAllocation` is used for discovering abstract topics within a collection of text documents through topic modeling.\n",
    "\n",
    "5. **For data visualization - [matplotlib](https://matplotlib.org/stable/gallery/index.html)**\n",
    "    - `matplotlib.pyplot as plt`: For basic plotting functionality\n",
    "\n",
    "6. **For formatting of display text**\n",
    "    - `textwrap` is used to format and wrap text for improved readability in output.\n",
    "\n",
    "7. **For mathematical operations**\n",
    "    - `math` supplies mathematical functions like ceiling, floor, and trigonometric operations.\n",
    "\n",
    "8. **For model persistence - [joblib](https://joblib.readthedocs.io/en/stable/)**\n",
    "    - `joblib` is used for saving and loading Python objects, such as machine learning models, to and from disk."
   ],
   "id": "66f261f8e762f93e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For File and system operations\n",
    "import urllib.request\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For text preprocessing\n",
    "import re\n",
    "\n",
    "# For topic modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For formatting of display text\n",
    "import textwrap\n",
    "\n",
    "# For mathematical operations\n",
    "import math\n",
    "\n",
    "# For model persistence\n",
    "import joblib"
   ],
   "id": "6c57cb96788ece17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Load the data",
   "id": "cb7408a28b69feaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose**: This chunk loads the dataset and displays a sample review.\n",
    "- Checks if the dataset file exists locally; if not, it downloads it from a specified URL\n",
    "- `iloc` is used for integer-location-based indexing, so customer_reviews_data['text'].iloc[0] retrieves the first row's review text.\n",
    "- The code then takes the first 100 characters of that review and appends \"...\" to indicate truncation."
   ],
   "id": "32cb2d71e0dcac7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_path = './data/processed_scaled_down_reviews.csv'\n",
    "url = \"https://github.com/course-files/SentimentAnalysis/raw/refs/heads/main/data/processed_scaled_down_reviews.csv\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "    urllib.request.urlretrieve(url, dataset_path)\n",
    "    print(\"✅ Dataset downloaded\")\n",
    "else:\n",
    "    print(\"✅ Dataset already exists locally\")\n",
    "\n",
    "customer_reviews_data = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "print(f\"\\nLoaded: {len(customer_reviews_data)} reviews\")\n",
    "print(\"Sample review:\")\n",
    "print(customer_reviews_data['text'].iloc[0][:100] + \"...\")"
   ],
   "id": "800bd5ccd84c3565",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Text Preprocessing",
   "id": "dc8fc6f7d5082544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk checks if the reviews are in English and cleans the text data.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) can work with text collections that include multiple languages, but there are important considerations to ensure meaningful and interpretable results.\n",
    "- **Fundamental Principle**: LDA is an unsupervised probabilistic model that discovers latent topics based on word co-occurrence patterns within documents. It does not inherently know what language the words belong to; it simply treats all tokens as part of a single vocabulary.\n",
    "\n",
    "- **Challenges with Multiple Languages**:\n",
    "  - **Separate vocabularies**: Words from different languages typically do not overlap, which can lead to the model identifying topics that correspond more to the language itself rather than to thematic content. For example, one topic might become dominated by English words, while another becomes dominated by Swahili words.\n",
    "  - **Stop words**: Each language has its own set of stop words (e.g., “the”, “and”, “is” in English; “na”, “kwa”, “ni” in Swahili). These need to be removed appropriately; otherwise, they can distort the topic distributions.\n",
    "  - **Semantic coherence**: The quality and interpretability of topics can be reduced because the same concept expressed in different languages might appear in different topics rather than being grouped together.\n",
    "\n",
    "- **Possible Solutions**:\n",
    "  - **Preprocessing**: Identify the language of each document or sentence and preprocess separately (including language-specific stop word removal, lemmatization, etc.).\n",
    "  - **Separate Models**: Train separate LDA models **for each language** and later map or align topics across languages.\n",
    "  - **Translation**: Translate all documents into a single language before applying LDA.\n",
    "  - **Multilingual embeddings**: Use multilingual word embeddings to bring words from different languages into a shared vector space before applying topic modeling (this typically involves more advanced models than standard LDA).\n",
    "\n",
    "- **Advanced Approaches**: There are **multilingual topic models** specifically designed to handle documents in multiple languages by aligning topics across languages, e.g.:\n",
    "  - **Multilingual LDA (MLDA)**\n",
    "  - **Polylingual Topic Model (PLTM)**\n",
    "\n",
    "---\n",
    "\n",
    "- `reset_index(drop=True)`\n",
    "- An index in a DataFrame uniquely identifies each row, allowing for efficient data selection, alignment, and retrieval. It acts as a label for rows, similar to row numbers, and is used for operations like filtering, joining, and resetting the order after filtering or modifying data.\n",
    "- `reset_index(drop=True)` resets the DataFrame’s index to a default integer index (0, 1, 2, ...), and with `drop=True`, it discards the old index instead of adding it as a new column. This is useful after filtering rows to ensure that the index remains sequential and clean.\n",
    "- A DataFrame in `pandas` automatically has an index (default: integer starting from 0). To create or set a custom index, use the set_index() method, e.g., `customer_reviews_data = customer_reviews_data.set_index('review_id')`\n",
    "\n",
    "---\n",
    "- Note that additional filtering can be done at this point. For example, if the business wants to focus on reviews for a particular hotel or a specific time period, you can filter the DataFrame accordingly."
   ],
   "id": "c8f4e51a8fa2a5c3",
   "attachments": {
    "fb81d385-ebfe-43d0-a2fa-fccb3d11d0f3.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABFQAAAWsCAIAAACIKUDHAAASaUlEQVR4XuzBAQEAAACAkP6v7ggKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA24MDEgAAAABB/1/3I1QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgI69DAAEPlETHAAAAAElFTkSuQmCC"
    },
    "b14db956-1987-414d-b715-02a2e25cdcb1.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABFQAAAWsCAIAAACIKUDHAAASaUlEQVR4XuzBAQEAAACAkP6v7ggKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA24MDEgAAAABB/1/3I1QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgI69DAAEPlETHAAAAAElFTkSuQmCC"
    },
    "2332d6cc-0a41-453c-9a18-e02443134887.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABFQAAAWsCAIAAACIKUDHAAASaUlEQVR4XuzBAQEAAACAkP6v7ggKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA24MDEgAAAABB/1/3I1QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgI69DAAEPlETHAAAAAElFTkSuQmCC"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "customer_reviews_data = customer_reviews_data[customer_reviews_data['is_english'] == True].reset_index(drop=True)\n",
    "print(f\"Filtered: The DataFrame now contains {len(customer_reviews_data)} reviews written using the English language\")"
   ],
   "id": "31655d7d9c1c566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk cleans the text data by removing non-alphabet characters and collapsing multiple spaces into a single space.\n",
    "- The `clean_text` function uses regular expressions to:\n",
    "  - Remove any character that is not a letter or space.\n",
    "  - Convert the text to lowercase.\n",
    "  - Collapse multiple spaces into a single space and strip leading/trailing spaces.\n",
    "    - This is important for ensuring that the text is in a consistent format for further analysis."
   ],
   "id": "c0b6966c74cdfa4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "customer_reviews_data['clean_text'] = customer_reviews_data['text'].apply(clean_text)"
   ],
   "id": "8a8106814250194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(customer_reviews_data[['text', 'clean_text', 'rating_overall']].head())",
   "id": "171e356e3350fa1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: Create the Document-Term Matrix (DTM)",
   "id": "c086fc1fc6d73e63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk converts the cleaned text into a document-term matrix (DTM) using the `CountVectorizer` from scikit-learn.\n",
    "\n",
    "- The DTM is a sparse matrix where each row represents a document (review) and each column represents a unique word (feature) from the corpus.\n",
    "- `max_features=1000` limits the vocabulary to the 1000 most frequent words, which helps reduce dimensionality and noise.\n",
    "- `stop_words='english'` removes common English words (like \"the\", \"is\", etc.) that do not contribute much to the meaning of the text.\n",
    "- `max_df=0.95` ignores words that appear in more than 95% of the documents, as they are likely too common to be useful.\n",
    "- `min_df=3` ignores words that appear in fewer than 3 documents, as they are too rare to be significant.\n",
    "- The resulting DTM is a sparse matrix where each row corresponds to a document and each column corresponds to a word in the vocabulary.\n",
    "- The shape of the DTM is printed to confirm its dimensions, which should be (number of documents, number of features).\n",
    "- The first 10 feature names (words) are printed to give an idea of the vocabulary used in the DTM.\n",
    "- The `CountVectorizer` is a powerful tool for converting text data into numerical features that can be used in machine learning models.\n",
    "- The DTM is a crucial step in text analysis, as it transforms unstructured text into a structured format that can be analyzed statistically.\n",
    "- The DTM can be used for various tasks such as topic modeling, sentiment analysis, and text classification.\n",
    "- The `CountVectorizer` is highly customizable, allowing for various preprocessing options such as tokenization, n-grams, and more.\n",
    "- The DTM can be further processed or transformed using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to weigh the importance of words in the context of the entire corpus.\n",
    "- The DTM can be saved to a file for later use or analysis, allowing for reproducibility and sharing of results.\n",
    "- The DTM can be visualized using techniques like word clouds or heatmaps to gain insights into the most frequent words and their distributions across documents.\n",
    "- The DTM can be used as input for machine learning algorithms, enabling tasks like clustering, classification, and regression on text data."
   ],
   "id": "b0c58ecf9942c2cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=3,\n",
    "    max_features=1000,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(customer_reviews_data['clean_text'])\n",
    "print(f\"\\n**The Document-Term Matrix has been created**\\n\")\n",
    "print(f\"Shape: {doc_term_matrix.shape} (documents x features)\")\n",
    "print(\"Sample features:\", vectorizer.get_feature_names_out()[:10])"
   ],
   "id": "5c8cf53f12abadf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Converts a \"slice\" of the DTM into a DataFrame for display\n",
    "dtm_sample = pd.DataFrame(\n",
    "    doc_term_matrix[:10, :8].toarray(),  # first 5 documents, first 10 terms\n",
    "    columns=vectorizer.get_feature_names_out()[:8]\n",
    ")\n",
    "display(dtm_sample)"
   ],
   "id": "df6977bdc1738005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: Design the Topic Model using the Latent Dirichlet Allocation (LDA) Algorithm",
   "id": "34972f8ec13f8070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk configures the Latent Dirichlet Allocation (LDA) model for topic modeling.\n",
    "- LDA is a generative probabilistic model that assumes each document is a mixture of topics, and each topic is a mixture of words.\n",
    "- `n_components=5` specifies the number of topics to discover in the dataset. This is a hyperparameter that can be tuned based on the dataset and analysis goals.\n",
    "- `learning_method='online'` uses an online learning algorithm, which is suitable for large datasets and allows the model to update incrementally.\n",
    "- `random_state=53` sets a random seed for reproducibility, ensuring that the results can be replicated.\n",
    "- `max_iter=9` limits the number of iterations for the model to converge. This can be adjusted based on the complexity of the dataset and the desired accuracy.\n",
    "- `n_jobs=-1` allows the model to use all available CPU cores for parallel processing, speeding up the training process.\n",
    "- The `LatentDirichletAllocation` class from scikit-learn is used to create the LDA model.\n",
    "- The model is then fitted to the document-term matrix (`doc_term_matrix`), which contains the preprocessed text data.\n",
    "- The `fit` method trains the LDA model on the document-term matrix, discovering the underlying topics in the text data.\n",
    "- After fitting, the model can be used to infer topics for new documents or to analyze the distribution of topics across the dataset.\n",
    "- The model can be evaluated using metrics like perplexity, which measures how well the model predicts the data. Lower perplexity indicates a better fit.\n",
    "- The trained model can be saved for later use, allowing for efficient topic inference on new data without retraining.\n",
    "- The LDA model can be used for various applications such as document clustering, topic discovery, and text summarization.\n",
    "- The LDA model can be fine-tuned by adjusting hyperparameters like the number of topics, learning rate, and regularization parameters to improve topic coherence and interpretability.\n",
    "- The model can be extended to include additional features like bigrams or trigrams, which can capture more context and improve topic quality.\n",
    "- The LDA model can be combined with other techniques like sentiment analysis to gain deeper insights into customer feedback and opinions."
   ],
   "id": "540391be7a3c874c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=5,\n",
    "    learning_method='online',\n",
    "    random_state=53,\n",
    "    max_iter=9,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lda.fit(doc_term_matrix)\n",
    "print(\"\\nModel training complete\")\n",
    "print(f\"Perplexity: {lda.perplexity(doc_term_matrix):.2f} (lower is better)\")"
   ],
   "id": "2923f46c603d8537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 6: Explore the Discovered Topics",
   "id": "bd31ef37cd995613"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk retrieves and prints the top words for each topic discovered by the LDA model.\n",
    "- `get_feature_names_out()` retrieves the feature names (words) from the `CountVectorizer` used to create the document-term matrix.\n",
    "- `model.components_` contains the topic-word distributions, where each row corresponds to a topic and each column corresponds to a word in the vocabulary.\n",
    "- `topic.argsort()[:-n_words-1:-1]` sorts the words in descending order of their importance to the topic and retrieves the indices of the top `n_words` words.\n",
    "- `top_words = [identified_feature_names[i] for i in top_idx]` retrieves the actual words corresponding to the top indices.\n",
    "- The function `print_topics` iterates through each topic, retrieves the top words, and prints them in a readable format.\n",
    "---\n",
    "- The topics are printed with their corresponding top words, providing insights into the themes present in the customer reviews."
   ],
   "id": "ebb1b5afa853db25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "identified_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_topics(model, provided_feature_names, n_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words = [provided_feature_names[i] for i in top_idx]\n",
    "        print(f\"Topic #{topic_idx+1}:\")\n",
    "        print(\"   \" + \" \".join(top_words))\n",
    "\n",
    "print(\"Discovered Topics:\")\n",
    "print_topics(lda, identified_feature_names)"
   ],
   "id": "e5298db1c6d2f88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 7: Visualize the Topic Distribution",
   "id": "8fe6f8ec7c2b7f77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk assigns the dominant topic to each document and visualizes the distribution of documents across topics.\n",
    "- `lda.transform(doc_term_matrix)` computes the topic distribution for each document in the document-term matrix.\n",
    "- `topic_results.argmax(axis=1)` retrieves the index of the topic with the highest probability for each document, effectively assigning the dominant topic to each document.\n",
    "- A bar chart is created to visualize the distribution of documents across topics.\n",
    "- `topic_counts = customer_reviews_data['dominant_topic'].value_counts().sort_index()` counts the number of documents assigned to each topic and sorts them by topic ID\n",
    "- `plot(kind='bar', color='skyblue', edgecolor='black')` creates a bar chart with specified colors and edge styles"
   ],
   "id": "d7c3fbbbfd053006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assign the dominant topic to each document\n",
    "topic_results = lda.transform(doc_term_matrix)\n",
    "customer_reviews_data['dominant_topic'] = topic_results.argmax(axis=1)\n",
    "\n",
    "# Plot document distribution across topics\n",
    "plt.figure(figsize=(10, 5))\n",
    "topic_counts = customer_reviews_data['dominant_topic'].value_counts().sort_index()\n",
    "topic_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title(\"Document Distribution Across Topics\", fontsize=14)\n",
    "plt.xlabel(\"Topic ID\", fontsize=12)\n",
    "plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add counts on bars\n",
    "for i, count in enumerate(topic_counts):\n",
    "    plt.text(i, count+5, str(count), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8fc3ab70eeede680",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 8: Visualize the top words in each topic",
   "id": "4b87a01c3bf3a06a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk visualizes the top words in each topic using horizontal bar charts.\n",
    "- `n_topics = model.n_components` retrieves the number of topics in the LDA model.\n",
    "- `n_cols = 3` and `n_rows = math.ceil(n_topics / n_cols)` determine the layout of the subplots, with 3 columns and enough rows to accommodate all topics.\n",
    "- `fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 5 * n_rows))` creates a grid of subplots for visualizing the top words in each topic.\n",
    "- The loop iterates through each topic, retrieves the top words and their weights, and creates a horizontal bar chart for each topic.\n",
    "- `ax.barh(y_pos, weights, color='skyblue', height=0.7, edgecolor='black')` creates a horizontal bar chart for the top words in the topic.\n",
    "- `ax.set_yticks(y_pos)` and `ax.set_yticklabels(top_words)` set the y-axis ticks and labels to the top words.\n",
    "- `ax.invert_yaxis()` inverts the y-axis to display the top word at the top of the chart.\n",
    "- `ax.set_title(f\"Topic {idx+1}\", fontsize=12)` sets the title for each subplot.\n",
    "- `ax.set_xlabel(\"Weight\", fontsize=10)` sets the x-axis label for each subplot.\n",
    "- `ax.text(v + 0.01, i, f\"{v:.2f}\", color='black', fontsize=9)` adds text annotations to the bars showing the weight of each word.\n",
    "- `plt.suptitle(\"Top Words per Topic\", fontsize=16)` sets a main title for the entire figure.\n",
    "- `plt.tight_layout(rect=[0, 0, 1, 0.95])` adjusts the layout to prevent overlap and ensure that the main title is visible.\n",
    "- The function `plot_topic_words` is defined to encapsulate this visualization logic, making it reusable for different models or datasets.\n",
    "- This visualization helps in understanding the key themes represented by each topic and the most significant words associated with them.\n",
    "- The horizontal bar charts provide a clear and intuitive way to compare the importance of different words within each topic."
   ],
   "id": "5ea8d9ca6e074a2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_topic_words(model, identified_feature_names, n_words=8):\n",
    "    n_topics = model.n_components\n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(n_topics / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 5 * n_rows))\n",
    "    axes = axes.flatten()  # Flatten in case of a single row\n",
    "\n",
    "    for idx in range(n_topics):\n",
    "        ax = axes[idx]\n",
    "        topic = model.components_[idx]\n",
    "        top_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words = [identified_feature_names[i] for i in top_idx]\n",
    "        weights = topic[top_idx]\n",
    "\n",
    "        y_pos = np.arange(len(top_words))\n",
    "        ax.barh(y_pos, weights, color='skyblue', height=0.7, edgecolor='black')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_words)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"Topic {idx+1}\", fontsize=12)\n",
    "        ax.set_xlabel(\"Weight\", fontsize=10)\n",
    "        for i, v in enumerate(weights):\n",
    "            ax.text(v + 0.01, i, f\"{v:.2f}\", color='black', fontsize=9)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(n_topics, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.suptitle(\"Top Words per Topic\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "plot_topic_words(lda, identified_feature_names)"
   ],
   "id": "9cd690f5ebacf2b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 9: Qualitative Analysis - Review Samples",
   "id": "1a01dded2682f010"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk shows representative reviews for each topic, providing qualitative insights into the themes identified by the LDA model.\n",
    "- The function `show_representative_reviews` takes a topic ID and the number of reviews to display as parameters.\n",
    "- `topic_docs = customer_reviews_data[customer_reviews_data['dominant_topic'] == topic_id]` filters the DataFrame to get all reviews associated with the specified topic.\n",
    "- `top_docs = topic_docs.iloc[topic_results[topic_docs.index, topic_id].argsort()[-n_reviews:]]` retrieves the reviews with the highest topic probability for the specified topic.\n",
    "- `get_topic_keywords` retrieves the top keywords for the specified topic using the LDA model and feature names.\n",
    "- The function prints the topic ID, key words, and representative reviews in a formatted manner.\n",
    "- `textwrap.fill(row['text'], width=80)` formats the review text to a specified width for better readability.\n",
    "- The function also iterates through the top reviews and prints them along with their ratings.\n",
    "- This qualitative analysis helps in understanding the context and sentiment behind the topics identified by the LDA model.\n",
    "- The representative reviews provide concrete examples of customer feedback related to each topic, allowing for deeper insights into customer opinions and experiences. The next step is to interpret and label the topics based on the words and sample reviews, which will provide a clearer understanding of what each topic represents.\n",
    "- This analysis can inform business decisions, such as improving services, addressing customer concerns, or enhancing marketing strategies.\n",
    "---\n",
    "- Note that the output showing the representative reviews can also be fed to an LLM (Large Language Model) to label the topics."
   ],
   "id": "40520bf6a1902134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_representative_reviews(topic_id, n_reviews=3):\n",
    "    topic_docs = customer_reviews_data[customer_reviews_data['dominant_topic'] == topic_id]\n",
    "\n",
    "    top_docs = topic_docs.iloc[topic_results[topic_docs.index, topic_id].argsort()[-n_reviews:]]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Representative Reviews for Topic #{topic_id+1}\")\n",
    "    print(f\"Key words: {', '.join(get_topic_keywords(lda, identified_feature_names, topic_id))}\")\n",
    "    print('='*60)\n",
    "\n",
    "    for i, (_, row) in enumerate(top_docs.iterrows()):\n",
    "        print(f\"\\nReview #{i+1} (Rating: {row['rating_overall']}/5):\")\n",
    "        print(textwrap.fill(row['text'], width=80))\n",
    "        print('-'*80)\n",
    "\n",
    "def get_topic_keywords(model, identified_feature_names, topic_id, n_words=5):\n",
    "    topic = model.components_[topic_id]\n",
    "    top_idx = topic.argsort()[:-n_words-1:-1]\n",
    "    return [identified_feature_names[i] for i in top_idx]\n",
    "\n",
    "for topic_id in range(5):\n",
    "    show_representative_reviews(topic_id)"
   ],
   "id": "474e8bf6442d4ebc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 10: Interpret and Label Topics with a Human-In-The-Loop Approach",
   "id": "90a20951014d89b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk provides suggested labels for the topics based on the top words and sample reviews.\n",
    "- The `topic_labels` dictionary maps each topic ID to a human-readable label that summarizes the theme of the topic.\n",
    "- The labels are based on the top words associated with each topic and the representative reviews displayed in the previous step.\n",
    "- The labels are designed to capture the essence of what each topic represents, making it easier to interpret the results of the topic modeling.\n",
    "- The labels can be used to communicate the findings to stakeholders, such as business managers or marketing teams, who may not be concerned with the technical details of topic modeling.\n",
    "- The next step can involve sentiment analysis to understand how customers feel about the topics identified, which can further inform business decisions and strategies."
   ],
   "id": "94600c15f3251dd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Label the topics based on the words, sample reviews, and overall rating\n",
    "topic_labels = {\n",
    "    0: \"Exceptional Guest Experience and Service Quality\",\n",
    "    1: \"Negative Service Experiences and Front Desk Issues\",\n",
    "    2: \"Great Location, Cleanliness, and Friendly Staff\",\n",
    "    3: \"Value for Money and Convenient City Location\",\n",
    "    4: \"Comfort and Practical Amenities for Short Stays\"\n",
    "}\n",
    "\n",
    "print(\"\\n**Suggested Topic Labels**\")\n",
    "for idx, label in topic_labels.items():\n",
    "    print(f\"Topic {idx+1}: {label}\")\n",
    "\n",
    "# Add labels to the DataFrame\n",
    "customer_reviews_data['topic_label'] = customer_reviews_data['dominant_topic'].map(topic_labels)\n",
    "\n",
    "print(\"\\n**Number of reviews per topic:**\")\n",
    "print(\"Frequency counts:\\n\", customer_reviews_data['topic_label'].value_counts())\n",
    "print(\"\\nPercentages:\\n\", customer_reviews_data['topic_label'].value_counts(normalize=True) * 100, \"%\")"
   ],
   "id": "df4f174db7fe3825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 11: Export the results for further analysis and reporting using a tool like Power BI",
   "id": "f9d30f19d8923aa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Purpose**: This chunk prepares the DataFrame for export by selecting relevant columns and saving it to a CSV file.\n",
    "- The DataFrame `customer_reviews_data` is filtered to include only the columns of interest: `text`, `rating_overall`, `clean_text`, and `topic_label`.\n",
    "- This ensures that the exported data contains the original review text, the overall rating, the cleaned text, and the assigned topic label.\n",
    "- The filtered DataFrame is then saved to a CSV file using the `to_csv` method, with `index=False` to exclude the index column from the output.\n",
    "- The output path is specified as `./data/processed_scaled_down_reviews_with_topics.csv`, which is a relative path indicating that the file will be saved in the `data` directory.\n",
    "- This export step is crucial for sharing the results with stakeholders or for integrating the topic modeling results into a larger data analysis workflow.\n",
    "- The saved CSV file can be easily imported into various data analysis tools, allowing for further exploration and visualization of the topics and customer feedback.\n",
    "---\n",
    "- Saving the model allows you to reuse the trained LDA model later without retraining, saving time and ensuring consistent results. This is useful for applying the same topic model to new or additional data, or for deploying the model in production.\n",
    "\n",
    "- **How to use a saved model**:\n",
    "\n",
    "- Load the model:\n",
    "    ```\n",
    "    import joblib\n",
    "    lda = joblib.load('./model/topic_model_lda.pkl')\n",
    "    ```\n",
    "- Transform new data:\n",
    "    - Preprocess and vectorize new text data using the same `CountVectorizer (with the same vocabulary)`\n",
    "    - Use `lda.transform(new_doc_term_matrix)` to get topic distributions for new documents\n",
    "    - This enables consistent topic assignments and analysis on new reviews or datasets"
   ],
   "id": "a4aa322d96e0c7af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the results as a CSV file for further analysis and reporting\n",
    "output_path = \"./data/processed_scaled_down_reviews_with_topics.csv\"\n",
    "# Ensure the data directory exists\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "# Save the CSV file regardless of environment\n",
    "customer_reviews_data[['date', 'offering_id', 'date_stayed', 'rating_overall', 'text', 'clean_text', 'topic_label']].to_csv(output_path, index=False)\n",
    "print(f\"\\n✅ Topic Modeling Results saved to {output_path}\")\n",
    "\n",
    "# Provide a download link if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "except ImportError:\n",
    "    print(\"❌ Not running in Google Colab, skipped dataset download link.\")\n",
    "\n",
    "# Save the trained LDA model\n",
    "model_path = './model/topic_model_lda.pkl'\n",
    "# Ensure the model directory exists\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "# Save the model regardless of environment\n",
    "joblib.dump(lda, model_path)\n",
    "print(f\"✅ The Topic Model saved to {model_path}\")\n",
    "\n",
    "# Provide a download link if running in Google Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(model_path)\n",
    "except ImportError:\n",
    "    print(\"❌ Not running in Google Colab, skipped model download link.\")"
   ],
   "id": "5b7565e82e5549b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "Alam, H., Ryu, W.-J., & Lee, S. (2016). Joint multi-grain topic sentiment: modeling semantic aspects for online reviews. Information Sciences, 339, 206-223. https://doi.org/10.1016/j.ins.2016.01.013"
   ],
   "id": "a4dcb338ee26a2eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
